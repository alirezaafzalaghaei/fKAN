{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYJyxBASBGKf"
   },
   "source": [
    "This notebook implementation incorporates elements from [this](https://keras.io/examples/nlp/text_classification_from_scratch/) Keras example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:14.308156Z",
     "iopub.status.busy": "2024-06-11T20:59:14.307967Z",
     "iopub.status.idle": "2024-06-11T20:59:15.712780Z",
     "shell.execute_reply": "2024-06-11T20:59:15.712373Z",
     "shell.execute_reply.started": "2024-06-11T20:59:14.308137Z"
    },
    "id": "ps65A3hsBGKj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:29:14.416398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from fkan.tensorflow import FractionalJacobiNeuralBlock as fJNB\n",
    "from tensorflow.keras import layers, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:15.713578Z",
     "iopub.status.busy": "2024-06-11T20:59:15.713295Z",
     "iopub.status.idle": "2024-06-11T20:59:15.717231Z",
     "shell.execute_reply": "2024-06-11T20:59:15.716089Z",
     "shell.execute_reply.started": "2024-06-11T20:59:15.713564Z"
    }
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:15.718809Z",
     "iopub.status.busy": "2024-06-11T20:59:15.718416Z",
     "iopub.status.idle": "2024-06-11T20:59:17.202657Z",
     "shell.execute_reply": "2024-06-11T20:59:17.202105Z",
     "shell.execute_reply.started": "2024-06-11T20:59:15.718760Z"
    },
    "id": "rjtnNlPeBGKo",
    "outputId": "7427148b-68f1-40e4-b680-80753207ed06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:29:16.151946: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 625\n",
      "Number of batches in raw_val_ds: 157\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:17.203388Z",
     "iopub.status.busy": "2024-06-11T20:59:17.203245Z",
     "iopub.status.idle": "2024-06-11T20:59:19.104847Z",
     "shell.execute_reply": "2024-06-11T20:59:19.104198Z",
     "shell.execute_reply.started": "2024-06-11T20:59:17.203374Z"
    },
    "id": "TjRgK0mOBGKo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:29:17.250630: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [20000]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-06-12 00:29:17.251123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [20000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 10000\n",
    "embedding_dim = 64\n",
    "sequence_length = 500\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:19.106096Z",
     "iopub.status.busy": "2024-06-11T20:59:19.105871Z",
     "iopub.status.idle": "2024-06-11T20:59:19.213547Z",
     "shell.execute_reply": "2024-06-11T20:59:19.212869Z",
     "shell.execute_reply.started": "2024-06-11T20:59:19.106072Z"
    },
    "id": "As611hRMBGKp"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:19.214385Z",
     "iopub.status.busy": "2024-06-11T20:59:19.214203Z",
     "iopub.status.idle": "2024-06-11T20:59:19.218054Z",
     "shell.execute_reply": "2024-06-11T20:59:19.217137Z",
     "shell.execute_reply.started": "2024-06-11T20:59:19.214371Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "epochs = 1\n",
    "q = 2  # See paper for the definition and role of q\n",
    "trial = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using a predefined Keras activation function, comment out `fJNB(q)` and uncomment `x = layers.Activation(activation=activation)(x)`, specifying your desired activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T20:59:19.221758Z",
     "iopub.status.busy": "2024-06-11T20:59:19.221431Z",
     "iopub.status.idle": "2024-06-11T21:00:33.509569Z",
     "shell.execute_reply": "2024-06-11T21:00:33.506274Z",
     "shell.execute_reply.started": "2024-06-11T20:59:19.221733Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:29:19.924715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_17' with dtype resource\n",
      "\t [[{{node Placeholder/_17}}]]\n",
      "2024-06-12 00:29:19.925014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_18' with dtype int64\n",
      "\t [[{{node Placeholder/_18}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.7673 - precision: 0.7600 - recall: 0.7798 - auc: 0.8686"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:30:15.545138: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_19' with dtype string\n",
      "\t [[{{node Placeholder/_19}}]]\n",
      "2024-06-12 00:30:15.546187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [5000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 58s 91ms/step - loss: 0.4440 - accuracy: 0.7673 - precision: 0.7600 - recall: 0.7798 - auc: 0.8686 - val_loss: 0.3242 - val_accuracy: 0.8610 - val_precision: 0.8155 - val_recall: 0.9366 - val_auc: 0.9465\n",
      "  1/782 [..............................] - ETA: 2:14 - loss: 0.2169 - accuracy: 0.9062 - precision: 0.9231 - recall: 0.8571 - auc: 0.9563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:30:18.298926: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_17' with dtype resource\n",
      "\t [[{{node Placeholder/_17}}]]\n",
      "2024-06-12 00:30:18.299800: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [25000]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 15s 19ms/step - loss: 0.3231 - accuracy: 0.8596 - precision: 0.8142 - recall: 0.9318 - auc: 0.9450\n"
     ]
    }
   ],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(500,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(64)(x)\n",
    "\n",
    "# x = layers.Activation(activation=activation)(x)\n",
    "x = fJNB(q)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        metrics.Precision(name=\"precision\"),\n",
    "        metrics.Recall(name=\"recall\"),\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=epochs, batch_size=batch_size, verbose=True\n",
    ")\n",
    "score = model.evaluate(test_ds, verbose=True)\n",
    "print(\n",
    "    history.history,\n",
    "    \"\\n\",\n",
    "    score,\n",
    "    file=open(\"sentiment-JNB%s-%d.txt\" % (str(q), trial), \"w\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_classification_from_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
